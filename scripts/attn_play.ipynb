{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor, load, baddbmm, zeros\n",
    "from torch.nn.functional import interpolate\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import gc\n",
    "from os.path import join\n",
    "\n",
    "@dataclass\n",
    "class Fixtures:\n",
    "  q_proj: FloatTensor\n",
    "  k_proj: FloatTensor\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def get_fixtures(\n",
    "  sigma: float,\n",
    "  head_dim: int,\n",
    "  key_length_factor: float,\n",
    "  key_tokens: int,\n",
    ") -> Fixtures:\n",
    "  \"\"\"\n",
    "  Imports pre-saved tensors from filesystem, by building a file path to identify which stage of which diffusion run we want to load.\n",
    "  Args:\n",
    "    sigma:\n",
    "      I believe discretized sigmas for 22-step Karras schedule *should* be:\n",
    "        14.6146, 11.9484, 9.7548, 7.9216, 6.3493, 5.0878, 4.0300, 3.1667, 2.4743, 1.9103, 1.4601, 1.1084, 0.8299, 0.6127, 0.4471, 0.3213, 0.2281, 0.1580, 0.1072, 0.0720, 0.0507, 0.0292\n",
    "      Yet, k-diffusion invoked my Unet with the following sigmas:\n",
    "        14.6147, 11.9776, 9.7593, 7.9029, 6.3579, 5.0793, 4.0277, 3.1686, 2.4716, 1.9104, 1.4621, 1.1072, 0.8289, 0.6128, 0.4469, 0.3211, 0.2270, 0.1576, 0.1072, 0.0713, 0.0463, 0.0292\n",
    "    head_dim:\n",
    "      https://twitter.com/Birchlabs/status/1609605588601675776\n",
    "      varies depending on which self-attn layer you're on (at least in SD1.5, lol). I have files for:\n",
    "      40, 80, 160, 160\n",
    "    key_length_factor:\n",
    "      1.0  = in-distribution image,     512x512, key length as large as 4096\n",
    "      2.25 = out-of-distribution image, 768x768, key length as large as 9216\n",
    "    key_tokens:\n",
    "      varies depending on which self-attn layer you're on, and how large your latents are\n",
    "        4096, 1024, 256, 64\n",
    "        9216, 2304, 576, 144\n",
    "  \"\"\"\n",
    "  root_dir='/home/birch/git/diffusers-play'\n",
    "  in_dir=join(root_dir, 'out_tensor')\n",
    "  tensor_path_prefix=join(in_dir, f'f{key_length_factor}_s{sigma:.4f}_k{key_tokens}_c{head_dim}')\n",
    "  q_proj: FloatTensor = load(f'{tensor_path_prefix}_q_proj.pt', weights_only=True, map_location=device)\n",
    "  k_proj: FloatTensor = load(f'{tensor_path_prefix}_k_proj.pt', weights_only=True, map_location=device)\n",
    "  return Fixtures(\n",
    "    q_proj=q_proj,\n",
    "    k_proj=k_proj,\n",
    "  )\n",
    "\n",
    "def get_attn_scores(\n",
    "  q_proj: FloatTensor,\n",
    "  k_proj: FloatTensor,\n",
    "  scale: float,\n",
    ") -> FloatTensor:\n",
    "  \"\"\"Computes (q_proj @ k_proj.T)*scale\"\"\"\n",
    "  # no bias, but baddbmm's API requires a tensor even if coefficient is 0\n",
    "  attn_bias: FloatTensor = zeros(1, 1, 1, dtype=q_proj.dtype, device=q_proj.device)\n",
    "  attn_scores: FloatTensor = baddbmm(\n",
    "    attn_bias,\n",
    "    q_proj,\n",
    "    k_proj.transpose(-1, -2),\n",
    "    # means don't apply bias\n",
    "    beta=0,\n",
    "    alpha=scale,\n",
    "  )\n",
    "  return attn_scores\n",
    "\n",
    "def softmax(x: FloatTensor, dim=-1) -> FloatTensor:\n",
    "  \"\"\"Typical softmax. same as PyTorch's built-in torch.Tensor.softmax(), but step-by-step in case you want to modify it.\"\"\"\n",
    "  maxes = x.max(dim, keepdim=True).values\n",
    "  diffs = x-maxes\n",
    "  del maxes\n",
    "  x_exp = diffs.exp()\n",
    "  del diffs\n",
    "  x_exp_sum = x_exp.sum(dim, keepdim=True)\n",
    "  quotient = x_exp/x_exp_sum\n",
    "  return quotient\n",
    "\n",
    "def topk_softmax(x: FloatTensor, k:int, dim=-1) -> FloatTensor:\n",
    "  \"\"\"Softmax with a modified denominator, which sums fewer elements (the topk only) to help you size it to a magnitude on which the model was trained.\"\"\"\n",
    "  maxes = x.max(dim, keepdim=True).values\n",
    "  diffs = x-maxes\n",
    "  del maxes\n",
    "  x_exp = diffs.exp()\n",
    "  x_exp_sum = x_exp.topk(k=k, dim=dim).values.sum(dim, keepdim=True)\n",
    "  quotient = x_exp/x_exp_sum\n",
    "  return quotient\n",
    "\n",
    "def resample_softmax(x: FloatTensor, k:int, dim=-1) -> FloatTensor:\n",
    "  \"\"\"Softmax with a modified denominator. for each query token: sorts attn_scores, resamples key dimension to size k; you can use this to increase/decrease denominator to the magnitude on which the model was trained.\"\"\"\n",
    "  maxes = x.max(dim, keepdim=True).values\n",
    "  diffs = x-maxes\n",
    "  del maxes\n",
    "  torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "  diffs_sorted = diffs.sort(dim=dim).values\n",
    "  x_exp = diffs.exp()\n",
    "  del diffs\n",
    "  # for downsampling:\n",
    "  #   mode='area' has best PSNR.. but maybe that's not important.\n",
    "  # for upsampling:\n",
    "  #   lerping between attn scores (mode='linear') feels reasonable\n",
    "  #   repeating attn scores (mode='nearest_exact') might be reasonable too\n",
    "  # not whether we'd care about anti-aliasing\n",
    "  diffs_resampled = interpolate(diffs_sorted, size=(*diffs_sorted.shape[:-1], k), mode='linear', antialias=False)\n",
    "  del diffs_sorted\n",
    "  diffs_exp_sum = diffs_resampled.exp().sum(dim, keepdim=True)\n",
    "  del diffs_resampled\n",
    "  quotient = x_exp/diffs_exp_sum\n",
    "  return quotient\n",
    "\n",
    "def resample_crude_softmax(x: FloatTensor, k:int, dim=-1) -> FloatTensor:\n",
    "  \"\"\"Softmax with a modified denominator. for each query token: resamples key dimension to size k; you can use this to increase/decrease denominator to the magnitude on which the model was trained.\"\"\"\n",
    "  maxes = x.max(dim, keepdim=True).values\n",
    "  diffs = x-maxes\n",
    "  del maxes\n",
    "  x_exp = diffs.exp()\n",
    "  diffs_resampled = interpolate(diffs, scale_factor=k/diffs.size(-1), mode='nearest-exact', antialias=False)\n",
    "  del diffs\n",
    "  diffs_exp_sum = diffs_resampled.exp().sum(dim, keepdim=True)\n",
    "  del diffs_resampled\n",
    "  quotient = x_exp/diffs_exp_sum\n",
    "  return quotient\n",
    "\n",
    "sigma = 14.6147\n",
    "head_dim = 40\n",
    "scale = head_dim ** -.5\n",
    "\n",
    "in_dist_key_tokens = 4096\n",
    "\n",
    "out_dist_key_tokens = 9216\n",
    "out_dist: Fixtures = get_fixtures(\n",
    "  sigma=sigma,\n",
    "  head_dim=head_dim,\n",
    "  key_length_factor = out_dist_key_tokens/in_dist_key_tokens, # 2.25\n",
    "  key_tokens = out_dist_key_tokens,\n",
    ")\n",
    "out_dist_attn_scores: FloatTensor = get_attn_scores(\n",
    "  q_proj=out_dist.q_proj,\n",
    "  k_proj=out_dist.k_proj,\n",
    "  scale=scale,\n",
    ")\n",
    "del out_dist\n",
    "# out_dist_attn_probs: FloatTensor = out_dist_attn_scores.softmax(dim=-1)\n",
    "out_dist_attn_probs: FloatTensor = softmax(out_dist_attn_scores, dim=-1)\n",
    "\n",
    "key_tokens: int = out_dist_attn_scores.size(-1)\n",
    "preferred_token_count: int = in_dist_key_tokens # 4096\n",
    "out_dist_topk_attn_probs: FloatTensor = topk_softmax(out_dist_attn_scores, k=preferred_token_count, dim=-1)\n",
    "\n",
    "# out_dist_resample_attn_probs: FloatTensor = resample_softmax(out_dist_attn_scores, k=preferred_token_count, dim=-1)\n",
    "# out_dist_resample_attn_probs: FloatTensor = resample_crude_softmax(out_dist_attn_scores, k=preferred_token_count, dim=-1)\n",
    "\n",
    "in_dist: Fixtures = get_fixtures(\n",
    "  sigma=sigma,\n",
    "  head_dim=head_dim,\n",
    "  key_length_factor = in_dist_key_tokens/in_dist_key_tokens, # 1.0\n",
    "  key_tokens = in_dist_key_tokens,\n",
    ")\n",
    "in_dist_attn_scores: FloatTensor = get_attn_scores(\n",
    "  q_proj=in_dist.q_proj,\n",
    "  k_proj=in_dist.k_proj,\n",
    "  scale=scale,\n",
    ")\n",
    "del in_dist\n",
    "# in_dist_attn_probs: FloatTensor = in_dist_attn_scores.softmax(dim=-1)\n",
    "in_dist_attn_probs: FloatTensor = softmax(in_dist_attn_scores, dim=-1)\n",
    "del in_dist_attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch import histogram\n",
    "from typing import NamedTuple\n",
    "\n",
    "class Histogram(NamedTuple):\n",
    "  boundaries: FloatTensor\n",
    "  counts: FloatTensor\n",
    "\n",
    "density=True\n",
    "bins=200\n",
    "hist, bin_edges = histogram(in_dist_attn_probs[0][0].log().float().cpu(), bins=bins, density=density)\n",
    "plt.fill_between(bin_edges[:-1], 0, hist, alpha=0.6, label='in-dist')\n",
    "hist, bin_edges = histogram(out_dist_attn_probs[0][0].log().float().cpu(), bins=bins, density=density)\n",
    "plt.fill_between(bin_edges[:-1], 0, hist, alpha=0.6, label='out-dist')\n",
    "# hist, bin_edges = histogram((out_dist_attn_probs[0][0]*(out_dist_key_tokens/in_dist_key_tokens)).log().float().cpu(), bins=bins, density=density)\n",
    "# plt.fill_between(bin_edges[:-1], 0, hist, alpha=0.6, label='out-dist (scaled)')\n",
    "# boundaries, counts = histogram(out_dist_topk_attn_probs[0][0].log().float().cpu(), bins)\n",
    "# plt.fill_between(bin_edges[:-1], 0, hist, alpha=0.6, label='out-dist (topk)')\n",
    "# boundaries, counts = histogram(out_dist_resample_attn_probs[0][0].log().float().cpu(), bins)\n",
    "# plt.fill_between(bin_edges[:-1], 0, hist, alpha=0.6, label='out-dist (resampled)')\n",
    "\n",
    "plt.title(f'Ïƒ={sigma} log self-attn probs for uncond batch, first down-block, head 0, query token 0')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
